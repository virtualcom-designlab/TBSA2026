
### The 1st International Workshop on Trust and Blame in Social Agents [(TBSA2026)](https://2026.persuasivetech.org/workshops-tutorials/#w2) in [Persuasive 2026](https://2026.persuasivetech.org/)

### important links:
- [program](program.md)
- [submission](submission.md)


## Organizers (names and affiliations):

- [Tomoko Yonezawa (Kansai University, Japan)](https://www.res.kutc.kansai-u.ac.jp/~yone/)
- [Ryo Uehara (University of Tokyo, Japan)](https://www.iii.u-tokyo.ac.jp/faculty/%E6%A4%8D%E5%8E%9F-%E4%BA%AE)
- [Kunifumi Saito (Keio University, Japan)](https://www.sfc.keio.ac.jp/faculty_profile/en/list/PM/kunifumi-saito.html)

<table border=0 align="center">
  <tr>
    <td align="center"><img src="https://www.ipa.go.jp/jinzai/mitou/it/2026/j5u9nn000000b77p-img/j5u9nn000000fs05.jpg" height="150px"></td>
    <td align="center"><img src="https://www.iii.u-tokyo.ac.jp/manage/wp-content/uploads/2025/06/9ca3f7266ed3935f0924c2d7e908d6b5-400x400.jpg" height="140px"></td>
    <td align="center"><img src="https://www.sfc.keio.ac.jp/faculty_profile/images/kunifumi23162.jpg" height="150px"></td>
  </tr>
  <tr>
    <td><a href="https://www.res.kutc.kansai-u.ac.jp/~yone/" target="_new">Tomoko Yonezawa (Kansai University, Japan)</a></td>
    <td><a href="https://www.iii.u-tokyo.ac.jp/faculty/%E6%A4%8D%E5%8E%9F-%E4%BA%AE" target="_new">Ryo Uehara (University of Tokyo, Japan)</a></td>
    <td><a href="https://www.sfc.keio.ac.jp/faculty_profile/en/list/PM/kunifumi-saito.html" target="_new">Kunifumi Saito (Keio University, Japan)</a></td>    
  </tr>
</table>



## Motivation: 

Social and virtual agents, including robots, are increasingly expected to directly/indirectly influence or persuade people across diverse contexts. This workshop investigates how social agents—including not only those designed to appear neutral or composed but also those with strongly emotional, low-fidelity, or minimalist embodiments—are perceived and treated as social actors, and how their roles intersect with ethics, trust, persuasion, blame, and responsibility. As human-like interfaces and embodied agents proliferate in domains such as caregiving, education, and customer service, they increasingly participate in emotionally, morally, and behaviorally influential interactions. This raises critical questions, such as:

- Whether we can meaningfully assign responsibility to such agents for their actions or errors
- How users calibrate trust or experience betrayal based on limited cues
- Who is blamed when things go wrong—the system, its designer, or the agent itself
- Under what conditions an agent should be allowed to refuse, avoid, or even sanction a user

The workshop aims to explore these questions through interdisciplinary perspectives spanning human–agent interaction (HAI1), philosophy of mind, affective computing, psychology, and interaction design. In doing so, we will address not only the technical and social mechanisms through which agency and social presence are perceived, but also the ethical legitimacy of granting or denying moral standing to these agents. We particularly welcome discussions on how ambiguity, unreliability, and minimal embodiment influence moral expectations and social responses such as persuasion in HAI. 
We also encourage reflection on the emotional and cultural boundaries of social perception, including how users emotionally respond to agent-initiated actions such as rejection, avoidance, or assertiveness—whether through confusion, frustration, or perceived betrayal. These reactions highlight the complex interplay of empathy, misattribution, and trust toward artificial agents, which may vary depending on embodiment fidelity and context. Furthermore, we consider how responsibility and moral legitimacy are differently assigned across cultural or regulatory contexts, and what technical, persuasive, or legal mechanisms might support or constrain such moral attributions.

## Focus and Objectives:

While traditional discussions of robot ethics have often centered on theoretical frameworks and idealized agents, this workshop seeks to expand the scope of ethical inquiry to include the complex, ambiguous, and emotionally nuanced contexts of real-world HAI. In particular, we aim to address the ethical, psychological, and persuasive challenges that arise when users engage with low-fidelity or minimal agents in in socially and emotionally charged situations, where blame, trust, influence, and responsibility often become blurred and culturally contingent. To this end, the workshop brings together interdisciplinary researchers and practitioners to examine how virtual agents and robots are positioned within ethical and persuasive frameworks, and how they shape or respond to users’ social expectations.
Key goals include:
+ Examine Attribution of Social/Moral Responsibility, and Persuasive Roles: Investigate how virtual agents and robots—especially those with low-fidelity embodiment—are perceived as social actors, and how users assign trust, blame, influence, or responsibility in ambiguous or emotionally charged situations.
+ Explore Design Implications of Ethically and Persuasively Charged Agent Behaviors: Discuss the legitimacy, risks, and design strategies for implementing agent behaviors such as rejection, avoidance, or punishment, and their ethical consequences for AI, HAI and CHI.
+ Foster Interdisciplinary Dialogue for Future Ethical and Persuasive Frameworks: Create a cross-disciplinary space bridging HRI, ethics, psychology, and persuasive AI design, to collaboratively rethink the conceptual and practical foundations of trust, influence, and agency in artificial systems.

We expect the workshop to result in:
-	A draft design principle and ethical guideline for socially and persuasively active agents capable of self-assertive interaction.
-	Building a new network of researchers interested in the intersection of social agency, persuasion, responsibility attribution, and trust in agents.





